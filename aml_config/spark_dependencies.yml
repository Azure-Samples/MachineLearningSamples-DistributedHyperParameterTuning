# Spark configuration and packages specification. The dependencies defined in
# this file will be automatically provisioned for each run that uses Spark.

# Spark configuration values can be set through the configuration dictionary.
# Spark packages can be added through the repositories and packages lists.

# For third-party python libraries, see conda_dependencies.yml.

configuration: 
  #"spark.driver.cores": "8"
  #"spark.driver.memory": "5200m"
  #"spark.executor.instances": "128"
  #"spark.executor.memory": "5200m"  
  #"spark.executor.cores": "2"
  
repositories:
  - "https://mmlspark.azureedge.net/maven"
  - "https://spark-packages.org/packages"
packages:
  - group: "com.microsoft.ml.spark"
    artifact: "mmlspark_2.11"
    version: "0.7.91"
  - group: "databricks"
    artifact: "spark-sklearn"
    version: "0.2.0"
